#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®Hugging Faceæ•°æ®ä¸‹è½½å™¨
å¿…é¡»åœ¨ç³»ç»ŸTerminalä¸­è¿è¡Œï¼Œä¸èƒ½åœ¨Cursorä¸­è¿è¡Œ

ä½¿ç”¨æ–¹æ³•:
    # ä¸‹è½½10ä¸ªæ–‡ä»¶æµ‹è¯•
    python download_real_hf_data.py --max-files 10
    
    # ä¸‹è½½50ä¸ªæ–‡ä»¶
    python download_real_hf_data.py --max-files 50
    
    # ä¸‹è½½å…¨éƒ¨
    python download_real_hf_data.py
"""

from huggingface_hub import hf_hub_download, list_repo_files, HfApi
import os
import sys
from pathlib import Path
import argparse


def test_connection():
    """æµ‹è¯•Hugging Faceè¿æ¥"""
    print("\nğŸ” æµ‹è¯•Hugging Faceè¿æ¥...")
    try:
        api = HfApi()
        api.whoami(token=None)
        print("âœ… è¿æ¥æˆåŠŸ\n")
        return True
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿åœ¨ç³»ç»ŸTerminalè¿è¡Œï¼ˆä¸æ˜¯Cursorï¼‰")
        print("2. é…ç½®ä»£ç†: export HTTPS_PROXY=http://proxy:port")
        print("3. ä½¿ç”¨é•œåƒ: export HF_ENDPOINT=https://hf-mirror.com")
        print("4. ç™»å½•: huggingface-cli login")
        return False


def download_biomedica_data(
    subset="commercial",
    max_files=None,
    output_dir="./biomedica_real_data"
):
    """
    ä¸‹è½½çœŸå®çš„BioMedICAæ•°æ®
    
    Args:
        subset: æ•°æ®é›†å­é›† (commercial, noncommercial, other)
        max_files: æœ€å¤šä¸‹è½½æ–‡ä»¶æ•° (None=å…¨éƒ¨)
        output_dir: è¾“å‡ºç›®å½•
    """
    
    repo_id = f"BIOMEDICA/biomedica_webdataset_{subset}"
    
    print("=" * 70)
    print("  ğŸš€ çœŸå®Hugging Faceæ•°æ®ä¸‹è½½å™¨")
    print("=" * 70)
    print(f"\nğŸ“¦ ä»“åº“: {repo_id}")
    print(f"ğŸ“ ç›®æ ‡: {output_dir}")
    print(f"ğŸ“Š é™åˆ¶: {max_files if max_files else 'å…¨éƒ¨'} ä¸ªæ–‡ä»¶")
    print("\n" + "-" * 70)
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“‹ è·å–æ–‡ä»¶åˆ—è¡¨...")
        files = list_repo_files(repo_id=repo_id, repo_type="dataset")
        tar_files = sorted([f for f in files if f.endswith('.tar')])
        
        if not tar_files:
            print(f"âŒ æœªæ‰¾åˆ°taræ–‡ä»¶åœ¨ä»“åº“ {repo_id}")
            return []
        
        print(f"âœ… æ‰¾åˆ° {len(tar_files)} ä¸ªtaræ–‡ä»¶")
        
        # ç¡®å®šä¸‹è½½æ•°é‡
        download_count = len(tar_files) if max_files is None else min(max_files, len(tar_files))
        
        print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½ {download_count} ä¸ªæ–‡ä»¶...")
        print("   (æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¯ä»¥éšæ—¶ä¸­æ–­)\n")
        
        # ä¸‹è½½æ–‡ä»¶
        downloaded = []
        failed = []
        
        for i, tar_file in enumerate(tar_files[:download_count]):
            print(f"[{i+1}/{download_count}] {tar_file}")
            
            try:
                local_path = hf_hub_download(
                    repo_id=repo_id,
                    filename=tar_file,
                    repo_type="dataset",
                    local_dir=output_dir,
                    resume_download=True
                )
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                size_mb = os.path.getsize(local_path) / (1024 * 1024)
                print(f"    âœ… å®Œæˆ ({size_mb:.1f} MB)")
                print(f"    ğŸ“ {local_path}\n")
                
                downloaded.append(local_path)
                
            except KeyboardInterrupt:
                print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
                print(f"å·²ä¸‹è½½: {len(downloaded)} ä¸ªæ–‡ä»¶")
                print("å¯ä»¥é‡æ–°è¿è¡Œå‘½ä»¤ç»§ç»­ä¸‹è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
                break
                
            except Exception as e:
                print(f"    âš ï¸  å¤±è´¥: {e}\n")
                failed.append(tar_file)
                continue
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 70)
        print("  ğŸ“Š ä¸‹è½½å®Œæˆ")
        print("=" * 70)
        print(f"âœ… æˆåŠŸ: {len(downloaded)} ä¸ªæ–‡ä»¶")
        if failed:
            print(f"âš ï¸  å¤±è´¥: {len(failed)} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ“ æ•°æ®ä½ç½®: {os.path.abspath(output_dir)}")
        
        # ç»Ÿè®¡æ€»å¤§å°
        total_size = sum(os.path.getsize(f) for f in downloaded)
        total_gb = total_size / (1024 ** 3)
        print(f"ğŸ’¾ æ€»å¤§å°: {total_gb:.2f} GB")
        
        print("\n" + "=" * 70)
        
        if downloaded:
            print("\nâœ… çœŸå®æ•°æ®ä¸‹è½½æˆåŠŸï¼ä¸æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼")
            print("\nğŸš€ ä¸‹ä¸€æ­¥:")
            print(f"\npython improved_cloud_training.py \\")
            print(f"    --local-path {output_dir} \\")
            print(f"    --epochs 10 \\")
            print(f"    --batch-size 32")
        
        return downloaded
        
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        print("\nğŸ”§ æ’æŸ¥æ­¥éª¤:")
        print("1. ç¡®è®¤åœ¨ç³»ç»ŸTerminalè¿è¡Œ: which python")
        print("2. æµ‹è¯•ç½‘ç»œ: curl -I https://huggingface.co")
        print("3. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯å¹¶é…ç½®ç½‘ç»œ")
        return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä¸‹è½½çœŸå®çš„BioMedICAæ•°æ®ï¼ˆå¿…é¡»åœ¨ç³»ç»ŸTerminalè¿è¡Œï¼‰'
    )
    parser.add_argument(
        '--subset', 
        default='commercial',
        choices=['commercial', 'noncommercial', 'other'],
        help='æ•°æ®é›†å­é›†'
    )
    parser.add_argument(
        '--max-files', 
        type=int, 
        default=10,
        help='æœ€å¤šä¸‹è½½æ–‡ä»¶æ•° (é»˜è®¤:10, 0=å…¨éƒ¨)'
    )
    parser.add_argument(
        '--output-dir', 
        default='./biomedica_real_data',
        help='è¾“å‡ºç›®å½•'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœmax_files=0ï¼Œä¸‹è½½å…¨éƒ¨
    max_files = None if args.max_files == 0 else args.max_files
    
    # è­¦å‘Šä¿¡æ¯
    print("\n" + "!" * 70)
    print("  âš ï¸  é‡è¦æç¤º")
    print("!" * 70)
    print("\næ­¤è„šæœ¬å¿…é¡»åœ¨ç³»ç»ŸTerminalä¸­è¿è¡Œï¼Œä¸èƒ½åœ¨Cursorä¸­è¿è¡Œï¼")
    print("\nå¦‚ä½•è¿è¡Œ:")
    print("  1. æ‰“å¼€Terminal.app (Command+Space â†’ terminal)")
    print("  2. cd /Users/yaohu/Desktop/advanced_vlm_training")
    print("  3. conda activate dataset_dwonload")
    print("  4. python download_real_hf_data.py --max-files 10")
    print("\n" + "!" * 70)
    
    # æµ‹è¯•è¿æ¥
    if not test_connection():
        print("\nâŒ æ— æ³•ç»§ç»­ï¼Œè¯·å…ˆè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜")
        sys.exit(1)
    
    # ç¡®è®¤ä¸‹è½½
    if max_files and max_files > 20:
        print(f"\nâš ï¸  å‡†å¤‡ä¸‹è½½ {max_files} ä¸ªæ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        response = input("ç¡®è®¤ç»§ç»­? (y/N): ")
        if response.lower() != 'y':
            print("å–æ¶ˆä¸‹è½½")
            sys.exit(0)
    
    # æ‰§è¡Œä¸‹è½½
    files = download_biomedica_data(
        subset=args.subset,
        max_files=max_files,
        output_dir=args.output_dir
    )
    
    if files:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()



