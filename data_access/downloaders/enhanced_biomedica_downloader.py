#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆBioMedICAæ•°æ®é›†ä¸‹è½½å™¨
ä»¿ç…§biomedica-etlé¡¹ç›®ï¼Œæ”¯æŒå®Œæ•´çš„ETLæµç¨‹

åŠŸèƒ½ç‰¹æ€§:
- æ”¯æŒWebDatasetå’ŒParquetä¸¤ç§æ ¼å¼
- å¹¶è¡Œä¸‹è½½å’Œåºåˆ—åŒ–
- æ•°æ®é¢„å¤„ç†å’Œè¿‡æ»¤
- æ”¯æŒHugging Face Hubå’Œç›´æ¥ä¸‹è½½
- å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œé”™è¯¯å¤„ç†

ä½¿ç”¨æ–¹æ³•:
    python enhanced_biomedica_downloader.py --format webdataset --subset commercial
    python enhanced_biomedica_downloader.py --format parquet --subset all
    python enhanced_biomedica_downloader.py --list-available
"""

import os
import sys
import json
import time
import logging
import argparse
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

# Hugging Faceç›¸å…³å¯¼å…¥
try:
    from huggingface_hub import snapshot_download, hf_hub_download, list_repo_files, HfApi
    from huggingface_hub.utils import HfHubHTTPError
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("è­¦å‘Š: huggingface_hubæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

# WebDatasetç›¸å…³å¯¼å…¥
try:
    import webdataset as wds
    WEBDATASET_AVAILABLE = True
except ImportError:
    WEBDATASET_AVAILABLE = False
    print("è­¦å‘Š: webdatasetæœªå®‰è£…ï¼ŒWebDatasetåŠŸèƒ½ä¸å¯ç”¨")

# Parquetç›¸å…³å¯¼å…¥
try:
    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
except ImportError:
    PARQUET_AVAILABLE = False
    print("è­¦å‘Š: pandas/pyarrowæœªå®‰è£…ï¼ŒParquetåŠŸèƒ½ä¸å¯ç”¨")


class EnhancedBioMedICADownloader:
    """å¢å¼ºç‰ˆBioMedICAæ•°æ®é›†ä¸‹è½½å™¨"""
    
    # å¯ç”¨çš„æ•°æ®é›†é…ç½®
    AVAILABLE_DATASETS = {
        "commercial": {
            "webdataset": "BIOMEDICA/biomedica_webdataset_commercial",
            "parquet": "BIOMEDICA/biomedica_parquet_commercial"
        },
        "noncommercial": {
            "webdataset": "BIOMEDICA/biomedica_webdataset_noncommercial", 
            "parquet": "BIOMEDICA/biomedica_parquet_noncommercial"
        },
        "other": {
            "webdataset": "BIOMEDICA/biomedica_webdataset_other",
            "parquet": "BIOMEDICA/biomedica_parquet_other"
        },
        "24m": {
            "webdataset": "BIOMEDICA/biomedica_webdataset_24M"
        }
    }
    
    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.config = self._load_config(config_file)
        self._setup_logging()
        self.hf_api = HfApi() if HF_AVAILABLE else None
        
    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "output_directory": "./biomedica_data",
            "max_retries": 5,
            "retry_delay": 15,
            "resume_download": True,
            "parallel_downloads": 4,
            "chunk_size": 1024 * 1024,  # 1MB
            "timeout": 300,
            "verify_checksum": True,
            "create_symlinks": False,
            "preprocessing": {
                "extract_images": True,
                "extract_captions": True,
                "filter_quality": True,
                "min_image_size": 224,
                "max_image_size": 2048
            }
        }
        
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_file}: {e}")
                print("ä½¿ç”¨é»˜è®¤é…ç½®")
        
        return default_config
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = Path(self.config['output_directory']) / 'logs'
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'enhanced_download.log', encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def list_available_datasets(self) -> Dict[str, Any]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        print("ğŸ” å¯ç”¨çš„BioMedICAæ•°æ®é›†:")
        print("=" * 60)
        
        for subset, formats in self.AVAILABLE_DATASETS.items():
            print(f"\nğŸ“Š {subset.upper()} å­é›†:")
            for format_type, repo_id in formats.items():
                print(f"  â€¢ {format_type}: {repo_id}")
        
        print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print(f"  â€¢ å•†ä¸šç”¨é€”: commercial")
        print(f"  â€¢ éå•†ä¸šç”¨é€”: noncommercial") 
        print(f"  â€¢ å…¶ä»–è®¸å¯: other")
        print(f"  â€¢ å®Œæ•´24Mæ•°æ®é›†: 24m")
        print(f"  â€¢ æ”¯æŒæ ¼å¼: webdataset, parquet")
        
        return self.AVAILABLE_DATASETS
    
    def check_dataset_availability(self, repo_id: str) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å¯ç”¨"""
        if not HF_AVAILABLE:
            self.logger.warning("Hugging Face Hubä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥æ•°æ®é›†å¯ç”¨æ€§")
            return False
            
        try:
            files = list_repo_files(repo_id=repo_id, repo_type="dataset")
            self.logger.info(f"æ•°æ®é›† {repo_id} å¯ç”¨ï¼ŒåŒ…å« {len(files)} ä¸ªæ–‡ä»¶")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®é›† {repo_id} ä¸å¯ç”¨: {e}")
            return False
    
    def download_webdataset(self, repo_id: str, output_dir: str) -> bool:
        """ä¸‹è½½WebDatasetæ ¼å¼çš„æ•°æ®é›†"""
        if not WEBDATASET_AVAILABLE:
            self.logger.error("WebDatasetåº“æœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½WebDatasetæ ¼å¼")
            return False
            
        self.logger.info(f"å¼€å§‹ä¸‹è½½WebDataset: {repo_id}")
        
        try:
            local_dir = snapshot_download(
                repo_id=repo_id,
                repo_type="dataset",
                local_dir=output_dir,
                resume_download=self.config['resume_download'],
                local_dir_use_symlinks=self.config['create_symlinks']
            )
            
            self.logger.info(f"âœ… WebDatasetä¸‹è½½å®Œæˆ: {local_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"WebDatasetä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def download_parquet(self, repo_id: str, output_dir: str) -> bool:
        """ä¸‹è½½Parquetæ ¼å¼çš„æ•°æ®é›†"""
        if not PARQUET_AVAILABLE:
            self.logger.error("Parquetåº“æœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½Parquetæ ¼å¼")
            return False
            
        self.logger.info(f"å¼€å§‹ä¸‹è½½Parquet: {repo_id}")
        
        try:
            local_dir = snapshot_download(
                repo_id=repo_id,
                repo_type="dataset", 
                local_dir=output_dir,
                resume_download=self.config['resume_download'],
                local_dir_use_symlinks=self.config['create_symlinks']
            )
            
            self.logger.info(f"âœ… Parquetä¸‹è½½å®Œæˆ: {local_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"Parquetä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def download_dataset(self, subset: str, format_type: str, output_dir: Optional[str] = None) -> bool:
        """ä¸‹è½½æŒ‡å®šå­é›†å’Œæ ¼å¼çš„æ•°æ®é›†"""
        if subset not in self.AVAILABLE_DATASETS:
            self.logger.error(f"æœªçŸ¥çš„å­é›†: {subset}")
            return False
            
        if format_type not in self.AVAILABLE_DATASETS[subset]:
            self.logger.error(f"å­é›† {subset} ä¸æ”¯æŒæ ¼å¼ {format_type}")
            return False
        
        repo_id = self.AVAILABLE_DATASETS[subset][format_type]
        
        if not self.check_dataset_availability(repo_id):
            return False
        
        if output_dir is None:
            output_dir = Path(self.config['output_directory']) / f"{subset}_{format_type}"
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if format_type == "webdataset":
            return self.download_webdataset(repo_id, str(output_dir))
        elif format_type == "parquet":
            return self.download_parquet(repo_id, str(output_dir))
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
            return False
    
    def download_multiple_datasets(self, datasets: List[Dict[str, str]]) -> bool:
        """å¹¶è¡Œä¸‹è½½å¤šä¸ªæ•°æ®é›†"""
        if not datasets:
            self.logger.error("æ²¡æœ‰æŒ‡å®šè¦ä¸‹è½½çš„æ•°æ®é›†")
            return False
        
        self.logger.info(f"å¼€å§‹å¹¶è¡Œä¸‹è½½ {len(datasets)} ä¸ªæ•°æ®é›†")
        
        success_count = 0
        with ThreadPoolExecutor(max_workers=self.config['parallel_downloads']) as executor:
            futures = []
            
            for dataset in datasets:
                subset = dataset['subset']
                format_type = dataset['format']
                output_dir = dataset.get('output_dir')
                
                future = executor.submit(self.download_dataset, subset, format_type, output_dir)
                futures.append((future, subset, format_type))
            
            for future, subset, format_type in futures:
                try:
                    success = future.result()
                    if success:
                        success_count += 1
                        self.logger.info(f"âœ… {subset}_{format_type} ä¸‹è½½æˆåŠŸ")
                    else:
                        self.logger.error(f"âŒ {subset}_{format_type} ä¸‹è½½å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"âŒ {subset}_{format_type} ä¸‹è½½å¼‚å¸¸: {e}")
        
        self.logger.info(f"æ‰¹é‡ä¸‹è½½å®Œæˆ: {success_count}/{len(datasets)} ä¸ªæ•°æ®é›†æˆåŠŸ")
        return success_count == len(datasets)
    
    def create_webdataset_loader(self, dataset_path: str) -> Optional[wds.WebDataset]:
        """åˆ›å»ºWebDatasetåŠ è½½å™¨"""
        if not WEBDATASET_AVAILABLE:
            self.logger.error("WebDatasetåº“æœªå®‰è£…")
            return None
        
        try:
            dataset = wds.WebDataset(dataset_path)
            self.logger.info(f"WebDatasetåŠ è½½å™¨åˆ›å»ºæˆåŠŸ: {dataset_path}")
            return dataset
        except Exception as e:
            self.logger.error(f"WebDatasetåŠ è½½å¤±è´¥: {e}")
            return None
    
    def create_parquet_loader(self, dataset_path: str) -> Optional[pd.DataFrame]:
        """åˆ›å»ºParquetåŠ è½½å™¨"""
        if not PARQUET_AVAILABLE:
            self.logger.error("Parquetåº“æœªå®‰è£…")
            return None
        
        try:
            df = pd.read_parquet(dataset_path)
            self.logger.info(f"Parquetæ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset_path}, å½¢çŠ¶: {df.shape}")
            return df
        except Exception as e:
            self.logger.error(f"ParquetåŠ è½½å¤±è´¥: {e}")
            return None
    
    def preprocess_data(self, input_path: str, output_path: str, format_type: str) -> bool:
        """æ•°æ®é¢„å¤„ç†"""
        self.logger.info(f"å¼€å§‹é¢„å¤„ç†æ•°æ®: {input_path} -> {output_path}")
        
        try:
            if format_type == "webdataset":
                return self._preprocess_webdataset(input_path, output_path)
            elif format_type == "parquet":
                return self._preprocess_parquet(input_path, output_path)
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
                return False
        except Exception as e:
            self.logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def _preprocess_webdataset(self, input_path: str, output_path: str) -> bool:
        """é¢„å¤„ç†WebDatasetæ•°æ®"""
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„WebDataseté¢„å¤„ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šå›¾åƒå°ºå¯¸è°ƒæ•´ã€è´¨é‡è¿‡æ»¤ã€æ ¼å¼è½¬æ¢ç­‰
        self.logger.info("WebDataseté¢„å¤„ç†åŠŸèƒ½å¾…å®ç°")
        return True
    
    def _preprocess_parquet(self, input_path: str, output_path: str) -> bool:
        """é¢„å¤„ç†Parquetæ•°æ®"""
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„Parqueté¢„å¤„ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šæ•°æ®æ¸…æ´—ã€ç‰¹å¾æå–ã€æ ¼å¼è½¬æ¢ç­‰
        self.logger.info("Parqueté¢„å¤„ç†åŠŸèƒ½å¾…å®ç°")
        return True
    
    def get_dataset_info(self, dataset_path: str, format_type: str) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        info = {
            "path": dataset_path,
            "format": format_type,
            "size": 0,
            "files": 0,
            "available": False
        }
        
        try:
            path = Path(dataset_path)
            if path.exists():
                info["available"] = True
                info["size"] = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
                info["files"] = len(list(path.rglob('*')))
                
                if format_type == "parquet" and PARQUET_AVAILABLE:
                    try:
                        df = pd.read_parquet(dataset_path)
                        info["rows"] = len(df)
                        info["columns"] = list(df.columns)
                    except:
                        pass
        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}")
        
        return info


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆBioMedICAæ•°æ®é›†ä¸‹è½½å™¨')
    parser.add_argument('--subset', type=str, choices=['commercial', 'noncommercial', 'other', '24m', 'all'],
                       help='æ•°æ®é›†å­é›†')
    parser.add_argument('--format', type=str, choices=['webdataset', 'parquet', 'both'],
                       help='æ•°æ®æ ¼å¼')
    parser.add_argument('--output', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--list-available', action='store_true', help='åˆ—å‡ºå¯ç”¨æ•°æ®é›†')
    parser.add_argument('--parallel', type=int, default=4, help='å¹¶è¡Œä¸‹è½½æ•°é‡')
    parser.add_argument('--preprocess', action='store_true', help='ä¸‹è½½åè¿›è¡Œé¢„å¤„ç†')
    parser.add_argument('--info', type=str, help='è·å–æŒ‡å®šæ•°æ®é›†ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = EnhancedBioMedICADownloader(args.config)
    
    try:
        if args.list_available:
            # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
            downloader.list_available_datasets()
            
        elif args.info:
            # è·å–æ•°æ®é›†ä¿¡æ¯
            info = downloader.get_dataset_info(args.info, "webdataset")
            print(f"æ•°æ®é›†ä¿¡æ¯: {json.dumps(info, indent=2, ensure_ascii=False)}")
            
        elif args.subset and args.format:
            # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
            if args.subset == 'all':
                # ä¸‹è½½æ‰€æœ‰å­é›†
                datasets = []
                for subset in ['commercial', 'noncommercial', 'other']:
                    if args.format == 'both':
                        for fmt in ['webdataset', 'parquet']:
                            datasets.append({
                                'subset': subset,
                                'format': fmt,
                                'output_dir': args.output
                            })
                    else:
                        datasets.append({
                            'subset': subset,
                            'format': args.format,
                            'output_dir': args.output
                        })
                
                success = downloader.download_multiple_datasets(datasets)
            else:
                success = downloader.download_dataset(args.subset, args.format, args.output)
            
            if success:
                print("âœ… ä¸‹è½½å®Œæˆ")
                if args.preprocess:
                    print("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é¢„å¤„ç†é€»è¾‘
            else:
                print("âŒ ä¸‹è½½å¤±è´¥")
                sys.exit(1)
        else:
            parser.print_help()
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
