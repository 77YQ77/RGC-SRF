#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BIOMEDICAæ··åˆè®­ç»ƒ - æœ€ç»ˆå®Œæ•´ç‰ˆ
æœ¬åœ°æ•°æ® + Hugging Faceæµå¼æ•°æ®

ç‰¹ç‚¹:
1. çº¯Pythonå®ç°ï¼ˆä¸éœ€è¦CLIå‘½ä»¤ï¼‰
2. æœ¬åœ°æ•°æ®ä¼˜å…ˆï¼ˆæ‚¨çš„10,653ä¸ªæ ·æœ¬ï¼‰
3. äº‘ç«¯æµå¼è¡¥å……ï¼ˆä»Hugging Faceå®æ—¶è¯»å–ï¼Œä¸ä¸‹è½½ï¼‰
4. Tokenè‡ªåŠ¨ç®¡ç†ï¼ˆç¯å¢ƒå˜é‡æˆ–å‚æ•°ï¼‰

ä½¿ç”¨æ–¹æ³•:
    export HF_TOKEN=hf_your_token_here
    python demo_hybrid_training.py --total-samples 50000 --epochs 10

å¿…é¡»åœ¨Terminal.appä¸­è¿è¡Œï¼
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from typing import Iterator, Dict, Any, Optional
import io

# è®¾ç½®ä»£ç†ï¼ˆå¿…é¡»ï¼ï¼‰
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ['http_proxy'] = 'http://127.0.0.1:7890'
os.environ['https_proxy'] = 'http://127.0.0.1:7890'

print("[ä»£ç†] ä»£ç†è®¾ç½®: http://127.0.0.1:7890")

# æ£€æŸ¥ä¾èµ–
print("[æ£€æŸ¥] æ£€æŸ¥ä¾èµ–...")

try:
    from huggingface_hub import hf_hub_url, login, HfApi, list_repo_files
    print("[OK] huggingface_hub")
except ImportError:
    print("[é”™è¯¯] ç¼ºå°‘ huggingface_hub")
    print("å®‰è£…: pip install huggingface_hub")
    sys.exit(1)

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.tensorboard import SummaryWriter
    import torchvision.transforms as transforms
    print("[OK] torch")
except ImportError:
    print("[é”™è¯¯] ç¼ºå°‘ torch")
    print("å®‰è£…: pip install torch torchvision")
    sys.exit(1)

try:
    import webdataset as wds
    print("[OK] webdataset")
except ImportError:
    print("[é”™è¯¯] ç¼ºå°‘ webdataset")
    print("å®‰è£…: pip install webdataset")
    sys.exit(1)

try:
    from PIL import Image
    print("[OK] PIL")
except ImportError:
    print("[é”™è¯¯] ç¼ºå°‘ pillow")
    print("å®‰è£…: pip install pillow")
    sys.exit(1)

try:
    from transformers import CLIPModel, CLIPProcessor
    print("[OK] transformers")
except ImportError:
    print("[é”™è¯¯] ç¼ºå°‘ transformers")
    print("å®‰è£…: pip install transformers")
    sys.exit(1)

try:
    from tqdm import tqdm
    print("[OK] tqdm")
except ImportError:
    print("[è­¦å‘Š]  å»ºè®®å®‰è£… tqdm: pip install tqdm")
    # tqdmä¸æ˜¯å¿…é¡»çš„ï¼Œå¯ä»¥ç»§ç»­
    tqdm = lambda x, **kwargs: x

print("\n[OK] æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼\n")


class HybridDataLoader:
    """æ··åˆæ•°æ®åŠ è½½å™¨ - æœ¬åœ° + HFæµå¼"""
    
    def __init__(self, local_path, hf_repo, total_samples, batch_size, hf_token):
        self.local_path = Path(local_path)
        self.hf_repo = hf_repo
        self.total_samples = total_samples
        self.batch_size = batch_size
        self.hf_token = hf_token
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # æ”¶é›†æœ¬åœ°taræ–‡ä»¶
        print(f"[ç›®å½•] æ‰«ææœ¬åœ°taræ–‡ä»¶: {local_path}")
        self.local_tar_files = self._collect_local_tar_files()
        print(f"[OK] æœ¬åœ°taræ–‡ä»¶: {len(self.local_tar_files)} ä¸ª")
        
        # ä¼°ç®—æ ·æœ¬æ•°
        self.local_count = len(self.local_tar_files) * 5000
        print(f"   ä¼°è®¡æ ·æœ¬: ~{self.local_count:,}")
        
        self._print_plan()
    
    def _collect_local_tar_files(self):
        """æ”¶é›†æœ¬åœ°taræ–‡ä»¶"""
        tar_files = []
        if not self.local_path.exists():
            return tar_files
        
        for tar_file in self.local_path.glob("*.tar"):
            tar_files.append(str(tar_file))
        
        return sorted(tar_files)
    
    def _print_plan(self):
        """æ‰“å°è®­ç»ƒè®¡åˆ’"""
        print("\n" + "=" * 70)
        print("  [æ•°æ®] æ··åˆè®­ç»ƒè®¡åˆ’")
        print("=" * 70)
        print(f"\n[è®­ç»ƒ] ç›®æ ‡: {self.total_samples:,} ä¸ªæ ·æœ¬")
        
        if self.local_count >= self.total_samples:
            print(f"\n[OK] æœ¬åœ°æ•°æ®å……è¶³")
            print(f"   ä½¿ç”¨: {self.total_samples:,} / {self.local_count:,} (100%æœ¬åœ°)")
        else:
            cloud_needed = self.total_samples - self.local_count
            local_pct = (self.local_count / self.total_samples) * 100
            cloud_pct = 100 - local_pct
            
            print(f"\n[Epoch] æ•°æ®åˆ†é…:")
            print(f"   [OK] æœ¬åœ°æ•°æ®: {self.local_count:,} ({local_pct:.1f}%)")
            print(f"   [äº‘ç«¯]  HFæµå¼: {cloud_needed:,} ({cloud_pct:.1f}%)")
            print(f"\n[å­˜å‚¨] èŠ‚çœå­˜å‚¨: ~{cloud_needed * 0.5 / 1024:.1f} GB")
            print(f"   (äº‘ç«¯æ•°æ®ä¸ä¸‹è½½ï¼Œç›´æ¥æµå¼è¯»å–)")
        
        print("=" * 70 + "\n")
    
    def _create_hf_stream_manual(self):
        """æ‰‹åŠ¨streaming - ä½¿ç”¨Python requestsé€ä¸ªè¯»å–tar"""
        print(f"[äº‘ç«¯]  è¿æ¥Hugging Face streaming...")
        
        try:
            import requests
            import tarfile
            
            # è·å–taræ–‡ä»¶åˆ—è¡¨
            files = list_repo_files(
                repo_id=self.hf_repo,
                repo_type="dataset",
                token=self.hf_token
            )
            tar_files = sorted([f for f in files if f.endswith('.tar')])
            print(f"   æ‰¾åˆ° {len(tar_files)} ä¸ªtaræ–‡ä»¶")
            
            # åªä½¿ç”¨å‰3ä¸ª
            max_shards = 3
            tar_files = tar_files[:max_shards]
            print(f"   [æç¤º] ä½¿ç”¨å‰ {len(tar_files)} ä¸ªæ–‡ä»¶\n")
            
            # æ„å»ºURLåˆ—è¡¨
            self.hf_tar_urls = []
            for f in tar_files:
                url = hf_hub_url(
                    repo_id=self.hf_repo,
                    filename=f,
                    repo_type="dataset"
                )
                self.hf_tar_urls.append(url)
            
            print(f"   [OK] å‡†å¤‡ {len(self.hf_tar_urls)} ä¸ªtaræ–‡ä»¶\n")
            return True
            
        except Exception as e:
            print(f"   [é”™è¯¯] å¤±è´¥: {e}")
            return False
    
    def _stream_from_tar_url(self, url):
        """ä»å•ä¸ªtar URL streamingè¯»å–æ ·æœ¬"""
        import requests
        import tarfile
        
        try:
            # ä½¿ç”¨ä»£ç†ä¸‹è½½
            proxies = {
                'http': 'http://127.0.0.1:7890',
                'https': 'http://127.0.0.1:7890'
            }
            
            headers = {'Authorization': f'Bearer {self.hf_token}'}
            
            print(f"   [ä¸‹è½½] Streaming: {url.split('/')[-1]}")
            
            response = requests.get(url, headers=headers, proxies=proxies, stream=True, timeout=60)
            response.raise_for_status()
            
            # ç”¨tarfileè¯»å–streamingå†…å®¹
            tar = tarfile.open(fileobj=response.raw, mode='r|*')
            
            for member in tar:
                if member.isfile():
                    # åªå¤„ç†jpgæ–‡ä»¶
                    if member.name.endswith('.jpg'):
                        # æå–å¯¹åº”çš„txtå’Œjson
                        base_name = member.name[:-4]
                        
                        try:
                            # è¯»å–jpg
                            jpg_file = tar.extractfile(member)
                            if jpg_file:
                                img = Image.open(jpg_file)
                                
                                # æŸ¥æ‰¾txtï¼ˆç®€åŒ–ï¼šå‡è®¾åœ¨åŒä¸€ä¸ªtarä¸­ï¼‰
                                txt_content = ""
                                
                                yield {
                                    'image': img,
                                    'text': txt_content,
                                    'metadata': {}
                                }
                        except:
                            continue
            
            tar.close()
            
        except Exception as e:
            print(f"   [è­¦å‘Š]  Tar streamingé”™è¯¯: {e}")
    
    def _create_local_stream(self):
        """åˆ›å»ºæœ¬åœ°taræ–‡ä»¶æµ"""
        if not self.local_tar_files:
            return None
        
        print(f"[ç›®å½•] åˆ›å»ºæœ¬åœ°taræµ...")
        print(f"   æœ¬åœ°tar: {len(self.local_tar_files)} ä¸ª")
        
        try:
            # ä½¿ç”¨å®˜æ–¹tutorialçš„DataPipeline
            dataset = wds.DataPipeline(
                wds.SimpleShardList(self.local_tar_files),
                wds.tarfile_to_samples(),
                wds.decode("pil"),
                wds.to_tuple("jpg", "txt", "json"),
            )
            
            print(f"   [OK] æœ¬åœ°tar Pipelineåˆ›å»ºå®Œæˆ\n")
            return dataset
            
        except Exception as e:
            print(f"   [é”™è¯¯] æœ¬åœ°taræµå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def __iter__(self):
        """è¿­ä»£å™¨ - æ··åˆæ¨¡å¼"""
        sample_count = 0
        local_used = 0
        cloud_used = 0
        
        # é˜¶æ®µ1: æœ¬åœ°taræ•°æ®
        print(f"[æ•°æ®] é˜¶æ®µ1: åŠ è½½æœ¬åœ°taræ•°æ®\n")
        
        local_stream = self._create_local_stream()
        
        if local_stream:
            for i, sample in enumerate(local_stream):
                if sample_count >= self.total_samples:
                    break
                
                try:
                    # tupleè§£åŒ…
                    img, caption, metadata = sample
                    
                    if not isinstance(img, Image.Image):
                        if isinstance(img, bytes):
                            img = Image.open(io.BytesIO(img))
                        else:
                            continue
                    
                    img = img.convert('RGB')
                    img_tensor = self.transform(img)
                    
                    if isinstance(caption, bytes):
                        caption = caption.decode('utf-8', errors='ignore')
                    
                    yield {
                        "image": img_tensor,
                        "text": str(caption),
                        "source": "local"
                    }
                    
                    sample_count += 1
                    local_used += 1
                    
                    if (i + 1) % 1000 == 0:
                        print(f"   æœ¬åœ°tar: {local_used:,}")
                        
                except Exception as e:
                    if local_used < 5:
                        print(f"   [è­¦å‘Š]  æœ¬åœ°æ ·æœ¬é”™è¯¯: {e}")
                    continue
        
        print(f"[OK] æœ¬åœ°taræ•°æ®: {local_used:,}\n")
        
        # é˜¶æ®µ2: HFæµå¼æ•°æ®
        if sample_count < self.total_samples:
            cloud_needed = self.total_samples - sample_count
            print(f"[æ•°æ®] é˜¶æ®µ2: HFæµå¼æ•°æ®")
            print(f"   è¿˜éœ€è¦: {cloud_needed:,}\n")
            
            # å‡†å¤‡HF tar URLs
            if self._create_hf_stream_manual():
                print(f"   [å¤„ç†] å¼€å§‹streamingè¯»å–...\n")
                
                # é€ä¸ªtaræ–‡ä»¶streaming
                for tar_url in self.hf_tar_urls:
                    if sample_count >= self.total_samples:
                        break
                    
                    # ä»è¿™ä¸ªtar URL streamingè¯»å–
                    for sample in self._stream_from_tar_url(tar_url):
                        if sample_count >= self.total_samples:
                            break
                        
                        try:
                            img = sample['image']
                            
                            if not isinstance(img, Image.Image):
                                continue
                            
                            img = img.convert('RGB')
                            img_tensor = self.transform(img)
                            
                            yield {
                                "image": img_tensor,
                                "text": sample.get('text', ''),
                                "source": "cloud"
                            }
                            
                            sample_count += 1
                            cloud_used += 1
                            
                            if cloud_used % 100 == 0:
                                print(f"   äº‘ç«¯: {cloud_used:,}/{cloud_needed:,}")
                        
                        except Exception as e:
                            if cloud_used < 5:
                                print(f"   [è­¦å‘Š]  æ ·æœ¬é”™è¯¯: {e}")
                            continue
                
                print(f"[OK] äº‘ç«¯streaming: {cloud_used:,}\n")
        
        print(f"[OK] æ€»è®¡: {sample_count:,}")
        print(f"   æœ¬åœ°: {local_used:,}")
        print(f"   äº‘ç«¯: {cloud_used:,}\n")
    
    def get_batches(self):
        """æ‰¹æ¬¡è¿­ä»£å™¨"""
        batch = []
        
        for sample in self:
            batch.append(sample)
            
            if len(batch) >= self.batch_size:
                yield {
                    "image": torch.stack([s["image"] for s in batch]),
                    "text": [s["text"] for s in batch],
                    "source": [s["source"] for s in batch]
                }
                batch = []
        
        if batch:
            yield {
                "image": torch.stack([s["image"] for s in batch]),
                "text": [s["text"] for s in batch],
                "source": [s["source"] for s in batch]
            }


def train_hybrid(args, hf_token):
    """æ··åˆè®­ç»ƒä¸»å‡½æ•° - æ ‡å‡†CLIPå¯¹æ¯”å­¦ä¹ """
    
    print("\n" + "=" * 70)
    print("  [å¼€å§‹] å¼€å§‹æ··åˆè®­ç»ƒï¼ˆæ ‡å‡†CLIPå¯¹æ¯”å­¦ä¹ ï¼‰")
    print("=" * 70)
    
    # è®¾ç½®æ¨¡å‹
    print("\n[æ•°æ®] è®¾ç½®CLIPæ¨¡å‹...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    try:
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        model = model.to(device)
        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        writer = SummaryWriter('./logs')
        print(f"[OK] æ¨¡å‹è®¾ç½®å®Œæˆ (è®¾å¤‡: {device})")
        print(f"   Vision Encoder: ViT-B/32")
        print(f"   Text Encoder: Transformer")
        print(f"   å‚æ•°é‡: ~149M")
    except Exception as e:
        print(f"[é”™è¯¯] æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print(f"\n[æ•°æ®] åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨...")
    try:
        dataloader = HybridDataLoader(
            local_path=args.local_path,
            hf_repo=args.hf_repo,
            total_samples=args.total_samples,
            batch_size=args.batch_size,
            hf_token=hf_token
        )
    except Exception as e:
        print(f"[é”™è¯¯] æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # è®­ç»ƒå¾ªç¯
    print(f"\n[è®­ç»ƒ] å¼€å§‹è®­ç»ƒ ({args.epochs} epochs)")
    print("=" * 70 + "\n")
    
    best_loss = float('inf')
    
    for epoch in range(args.epochs):
        print(f"[Epoch] Epoch {epoch + 1}/{args.epochs}")
        
        model.train()
        epoch_loss = 0
        batch_count = 0
        epoch_local = 0
        epoch_cloud = 0
        
        try:
            for batch_idx, batch in enumerate(dataloader.get_batches()):
                # ç»Ÿè®¡æ¥æº
                for src in batch['source']:
                    if src == 'local':
                        epoch_local += 1
                    else:
                        epoch_cloud += 1
                
                # å‡†å¤‡æ•°æ®
                images = batch['image']  # tensorå·²ç»å¤„ç†å¥½
                texts = batch['text']    # list of strings
                
                # è½¬æ¢ä¸ºPILå›¾åƒï¼ˆprocessoréœ€è¦ï¼‰
                from torchvision.transforms import ToPILImage
                to_pil = ToPILImage()
                pil_images = [to_pil(img) for img in images]
                
                # ä½¿ç”¨CLIP processorå¤„ç†
                inputs = processor(
                    text=texts,
                    images=pil_images,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=77
                ).to(device)
                
                # å‰å‘ä¼ æ’­ - æ ‡å‡†CLIP
                outputs = model(**inputs)
                
                # CLIPå¯¹æ¯”å­¦ä¹ æŸå¤±
                logits_per_image = outputs.logits_per_image  # [batch, batch]
                logits_per_text = outputs.logits_per_text    # [batch, batch]
                
                # æ ‡ç­¾ï¼šå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
                labels = torch.arange(len(pil_images)).to(device)
                
                # åŒå‘å¯¹æ¯”æŸå¤±
                loss_i2t = torch.nn.functional.cross_entropy(logits_per_image, labels)
                loss_t2i = torch.nn.functional.cross_entropy(logits_per_text, labels)
                
                # æ€»æŸå¤±
                loss = (loss_i2t + loss_t2i) / 2
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                
                epoch_loss += loss.item()
                batch_count += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if batch_idx % 10 == 0:
                    avg_loss = epoch_loss / batch_count
                    print(f"   Batch {batch_idx}: Loss={loss.item():.4f}, "
                          f"I2T={loss_i2t.item():.4f}, T2I={loss_t2i.item():.4f}, "
                          f"Local={epoch_local}, Cloud={epoch_cloud}")
                
                # ä¸ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœç©ºé—´ï¼‰
                # åªåœ¨æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
        
        except StopIteration:
            pass
        except Exception as e:
            print(f"[è­¦å‘Š]  Epochè®­ç»ƒå‡ºé”™: {e}")
            continue
        
        # Epochæ€»ç»“
        avg_loss = epoch_loss / max(batch_count, 1)
        print(f"\n[æ•°æ®] Epoch {epoch+1} å®Œæˆ:")
        print(f"   æŸå¤±: {avg_loss:.4f}")
        print(f"   æœ¬åœ°: {epoch_local:,}")
        print(f"   äº‘ç«¯: {epoch_cloud:,}\n")
        
        # TensorBoardè®°å½•
        writer.add_scalar('Loss/Total', avg_loss, epoch)
        writer.add_scalar('Samples/Local', epoch_local, epoch)
        writer.add_scalar('Samples/Cloud', epoch_cloud, epoch)
        writer.add_scalar('Samples/Total', epoch_local + epoch_cloud, epoch)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåªä¿ç•™æœ€å¥½çš„ä¸€ä¸ªï¼‰
        if avg_loss < best_loss:
            best_loss = avg_loss
            Path("./models").mkdir(exist_ok=True)
            
            # åˆ é™¤æ—§æ¨¡å‹
            import glob
            for old_model in glob.glob("./models/best_model_*.pt"):
                try:
                    os.remove(old_model)
                except:
                    pass
            
            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'loss': avg_loss
            }, f"./models/best_model_e{epoch}.pt")
            print(f"   [ä¿å­˜] ä¿å­˜æœ€ä½³æ¨¡å‹ (Loss: {avg_loss:.4f})\n")
    
    writer.close()
    
    print("=" * 70)
    print("  [å®Œæˆ] è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"[æœ€ä½³] æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"[ç›®å½•] æ¨¡å‹: ./models/")
    print(f"[Epoch] æ—¥å¿—: tensorboard --logdir=./logs")
    
    return True


def main():
    parser = argparse.ArgumentParser(description='BIOMEDICAæ··åˆè®­ç»ƒ')
    parser.add_argument('--total-samples', type=int, default=50000)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=256)
    parser.add_argument('--local-path', type=str, default='./dataset')
    parser.add_argument('--hf-repo', type=str, default='BIOMEDICA/biomedica_webdataset_24M')
    parser.add_argument('--hf-token', type=str, default=None)
    
    args = parser.parse_args()
    
    # è·å–token
    hf_token = args.hf_token or os.environ.get('HF_TOKEN')
    
    if not hf_token:
        print("\n[é”™è¯¯] æœªæ‰¾åˆ°Hugging Face token")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  export HF_TOKEN=hf_your_token")
        print("  python å®Œæ•´æ··åˆè®­ç»ƒ_æœ€ç»ˆç‰ˆ.py")
        print("\næˆ–:")
        print("  python å®Œæ•´æ··åˆè®­ç»ƒ_æœ€ç»ˆç‰ˆ.py --hf-token hf_your_token")
        sys.exit(1)
    
    # ç™»å½•éªŒè¯
    print(f"\nğŸ” ç™»å½•Hugging Face...")
    print(f"Token: {hf_token[:15]}...{hf_token[-10:]}")
    
    try:
        login(token=hf_token, add_to_git_credential=False)
        
        api = HfApi()
        user = api.whoami(token=hf_token)
        print(f"[OK] ç™»å½•æˆåŠŸ: {user.get('name', 'User')}")
        
        # éªŒè¯æ•°æ®é›†è®¿é—®
        print(f"\n[æ£€æŸ¥] éªŒè¯æ•°æ®é›†è®¿é—®...")
        files = list_repo_files(args.hf_repo, repo_type="dataset", token=hf_token)
        print(f"[OK] å¯è®¿é—®: {args.hf_repo}")
        print(f"   æ–‡ä»¶æ•°: {len(files)}")
        
    except Exception as e:
        print(f"[é”™è¯¯] ç™»å½•/éªŒè¯å¤±è´¥: {e}")
        print(f"\nè¯·ç¡®ä¿:")
        print(f"1. Tokenæœ‰æ•ˆ")
        print(f"2. å·²åŒæ„æ¡æ¬¾: https://huggingface.co/datasets/{args.hf_repo}")
        print(f"3. åœ¨Terminal.appè¿è¡Œï¼ˆä¸æ˜¯Cursorï¼‰")
        sys.exit(1)
    
    # å¼€å§‹è®­ç»ƒ
    success = train_hybrid(args, hf_token)
    
    if success:
        print("\n[OK] æˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\n[é”™è¯¯] è®­ç»ƒå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    print("\n" + "!" * 70)
    print("  [è­¦å‘Š]  é‡è¦æç¤º")
    print("!" * 70)
    print("\nå¿…é¡»åœ¨Terminal.appä¸­è¿è¡Œï¼ˆä¸æ˜¯Cursorç»ˆç«¯ï¼‰ï¼")
    print("\nå¿…é¡»å…ˆ:")
    print("  1. åŒæ„æ¡æ¬¾: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M")
    print("  2. è®¾ç½®Token: export HF_TOKEN=hf_your_token_here")
    print("\n" + "!" * 70 + "\n")
    
    main()

